version: "3.8"
services:
  ollama-train:
    build: .
    container_name: ollama-train
    runtime: nvidia
    env_file:
      - .env

    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      # If you run Ollama on the host and want container to access its models directory, mount it:
      - /home/you/.ollama:/root/.ollama
    restart: unless-stopped
